"""
IRIS v6.0 - Local AI Provider
Regelbaserad lokal AI utan externa API:er
"""

import logging
from typing import Dict, Any, AsyncIterator
from .base import BaseAIProvider

logger = logging.getLogger(__name__)

class LocalProvider(BaseAIProvider):
    """
    Lokal regelbaserad AI provider
    Anv칛nds f칬r privat profil och som sista fallback
    """
    
    def __init__(self):
        logger.info("游눹 LocalProvider initialiserad")
    
    def get_provider_name(self) -> str:
        return "lokal"
    
    async def analyze(
        self,
        query: str,
        context: str,
        model: str = "lokal",
        temperature: float = 0.0,
        max_tokens: int = 1000,
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        Enkel lokal analys utan externa API:er
        
        Args:
            query: Anv칛ndarens fr친ga
            context: Kontext fr친n datak칛llor
            model: AI-modell att anv칛nda
            temperature: Kreativitetsniv친 (0.0-1.0)
            max_tokens: Max antal tokens i svaret
            stream: Om streaming ska anv칛ndas
            
        Returns:
            Dict med svar, modell, provider, etc.
        """
        try:
            # Input validation
            if query is None:
                query = ""
            
            if context is None:
                context = ""
            
            logger.info("游눹 Anv칛nder lokal regelbaserad analys")
            
            response_parts = [
                f"Baserat p친 din fr친ga '{query}' och tillg칛ngliga svenska k칛llor:"
            ]
            
            # Analysera kontext
            if context:
                context_lower = context.lower()
                
                if "omx" in context_lower or "finansiell" in context_lower:
                    response_parts.append("- Finansiell data fr친n OMX Stockholm visar aktuell b칬rsaktivitet.")
                
                if "scb" in context_lower or "statistik" in context_lower:
                    response_parts.append("- Statistik fr친n SCB ger officiella svenska siffror.")
                
                if "smhi" in context_lower or "v칛der" in context_lower:
                    response_parts.append("- V칛derdata fr친n SMHI ger prognoser f칬r Sverige.")
                
                if "nyheter" in context_lower or "news" in context_lower:
                    response_parts.append("- Aktuella nyheter fr친n svenska medier.")
            
            # L칛gg till info om begr칛nsningar
            response_parts.append("\nOBS: Detta 칛r en lokal regelbaserad analys.")
            response_parts.append("F칬r mer detaljerad AI-analys, anv칛nd 'snabb' eller 'smart' profil med externa AI-providers.")
            
            # Approximera tokens anv칛nda (anta ~4 tecken per token)
            full_response = "\n".join(response_parts)
            estimated_tokens = len(full_response) // 4
            
            return {
                "svar": full_response,
                "modell": model,  # Anv칛nd modell-parametern
                "provider": "lokal",
                "typ": "rule_based",
                "tokens_used": estimated_tokens
            }
        except Exception as e:
            logger.error(f"Lokal analys fel: {e}", exc_info=True)
            raise
    
    async def analyze_stream(
        self,
        query: str,
        context: str,
        model: str = "lokal",
        temperature: float = 0.0,
        max_tokens: int = 1000
    ) -> AsyncIterator[str]:
        """
        Lokal streaming - yield:ar hela svaret
        
        Args:
            query: Anv칛ndarens fr친ga
            context: Kontext fr친n datak칛llor
            model: AI-modell att anv칛nda
            temperature: Kreativitetsniv친
            max_tokens: Max antal tokens
            
        Yields:
            Chunks av svaret
        """
        try:
            result = await self.analyze(query, context, model, temperature, max_tokens, stream=True)
            yield result["svar"]
        except Exception as e:
            logger.error(f"Lokal streaming fel: {e}", exc_info=True)
            raise
